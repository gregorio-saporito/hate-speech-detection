{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPI1+ivlBI4sZn7LkJORi7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregorio-saporito/hate-speech-detection/blob/main/main4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d97S5NUklYiU"
      },
      "source": [
        "# Text Mining and Sentiment Analysis: Hate Speech Detection\n",
        "Gregorio Luigi Saporito - DSE (2020-2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxNU63ovlc_y"
      },
      "source": [
        "### Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myJO2kwJk88I"
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "import random as rn\n",
        "rn.seed(123)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeVdBIirlvYY"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbf5u9I_lxec",
        "outputId": "13ad5092-9e59-4400-b885-1eda94d37884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install text-preprocessing\n",
        "!pip install tweet-preprocessor\n",
        "!pip install -U nltk[twitter]\n",
        "!pip install joblib"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: text-preprocessing in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: names-dataset in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (2.0.1)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (0.6.2)\n",
            "Requirement already satisfied: unittest-xml-reporting in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (3.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (3.6.2)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (0.0.48)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (7.1.2)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions->text-preprocessing) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions->text-preprocessing) (0.2.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions->text-preprocessing) (1.4.2)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already up-to-date: nltk[twitter] in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk[twitter]) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk[twitter]) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk[twitter]) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk[twitter]) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: twython; extra == \"twitter\" in /usr/local/lib/python3.7/dist-packages (from nltk[twitter]) (3.8.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython; extra == \"twitter\"->nltk[twitter]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython; extra == \"twitter\"->nltk[twitter]) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython; extra == \"twitter\"->nltk[twitter]) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython; extra == \"twitter\"->nltk[twitter]) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython; extra == \"twitter\"->nltk[twitter]) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython; extra == \"twitter\"->nltk[twitter]) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython; extra == \"twitter\"->nltk[twitter]) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGr8WAZwmCHT"
      },
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import preprocessor as p\n",
        "from text_preprocessing import preprocess_text\n",
        "from text_preprocessing import to_lower, expand_contraction, remove_number, remove_punctuation, remove_whitespace, normalize_unicode, lemmatize_word\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from sklearn import utils\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import statistics as st\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif, chi2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from mlxtend.feature_selection import ColumnSelector\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viJwoyKAmZ1n",
        "outputId": "9e331e7b-a57c-4e9d-e1d6-1968ab9aa773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}